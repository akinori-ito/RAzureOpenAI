% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/azureopenai.R
\name{AzureOpenAI_get_response}
\alias{AzureOpenAI_get_response}
\title{AzureOpenAI_get_response: Get Chat Response from Azure OpenAI API
This function obtains response from Azure OpenAI API.}
\usage{
AzureOpenAI_get_response(prompt, env, role = "user", max_tokens = 100)
}
\arguments{
\item{prompt}{Either a string or a list. When it is a string, the role
the prompt is assumed to be a user. If it is a list, it should be a
list of list(role, prompt), such as
list(list(role="system", content="You are a machine"),
list(role="user",content="Hi, machine!")).}

\item{env}{A list of environment, which contains an API key and the endpoint URL:
list(api_key="API_KEY",
endpoint="https://ENDPOINT/openai/deployments/MODELNAME/chat/completions?api-version=API_VERSION")}

\item{max_tokens}{Maximum number of tokens}
}
\value{
A string that contains the response from the LLM
}
\description{
AzureOpenAI_get_response: Get Chat Response from Azure OpenAI API
This function obtains response from Azure OpenAI API.
}
\examples{
api_key <- "API_KEY"
endpoint <- "https://ENDPOINT/openai/deployments/MODELNAME/chat/completions?api-version=API_VERSION"
environ <- list(
 api_key=api_key,
 endpoint=endpoint
)
prompt <- "List 10 foods good for eat with coke"
resp <- AzureOpenAI_get_response(prompt,environ,max_tokens=1000)
print(resp)
}
